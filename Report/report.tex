%%%%%%%% ICML 2025 STYLE REPORT %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended packages
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Remove [accepted] to avoid "Proceedings of..." text
\usepackage{icml2025}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Define argmin
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[capitalize,noabbrev]{cleveref}

% Algorithm packages
\usepackage{algorithm}
\usepackage{algorithmic}

% Theorems
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\icmltitlerunning{Prototype Selection for 1-NN Classification}

\begin{document}

\twocolumn[
\icmltitle{Variance-Weighted Centroid Selection for \\
           Prototype-Based 1-Nearest Neighbor Classification}

\begin{icmlauthorlist}
\icmlauthor{Wayne Wang}{ucsd}
\end{icmlauthorlist}

\icmlaffiliation{ucsd}{Department of Computer Science and Engineering, University of California San Diego, La Jolla, CA, USA}

\icmlcorrespondingauthor{Wayne Wang}{waw009@ucsd.edu}

\icmlkeywords{Prototype Selection, Nearest Neighbor, MNIST, Clustering}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
We address the problem of selecting a small set of representative prototypes from a large training set for 1-nearest neighbor (1-NN) classification. We propose a variance-weighted centroid selection algorithm that allocates prototypes proportionally based on within-class variance and uses K-Means clustering to identify representative samples. Experiments on MNIST demonstrate that our method significantly outperforms random selection, with advantages becoming more pronounced at higher compression ratios. At 60$\times$ compression (1,000 prototypes from 60,000 training samples), our method achieves 92.47\% accuracy compared to 88.56\% for random selection (+3.91\%). At extreme compression (6,000$\times$, only 10 prototypes), our method maintains 67.01\% accuracy while random selection degrades to 39.37\% (+27.64\%).
\end{abstract}

%============================================================================
% SECTION 1: High-level description (Requirement 1)
%============================================================================
\section{Introduction and Key Idea}
\label{sec:intro}

The $k$-nearest neighbor ($k$-NN) algorithm is a fundamental non-parametric classifier that predicts the label of a test sample based on the labels of its $k$ closest training samples \cite{cover1967nearest}. \textbf{Prototype selection} addresses the computational limitation of $k$-NN by selecting a representative subset of training samples to use for classification.

\subsection{Key Idea (High-Level Description)}

Our prototype selection method is based on two key observations:

\begin{enumerate}
    \item \textbf{Good prototypes are typical samples, not boundary samples.} While decision boundaries are defined by samples near class interfaces, the best prototypes are cluster centroids that represent the ``typical'' appearance of each class. Boundary points are often noisy or ambiguous.

    \item \textbf{Classes with higher variance need more prototypes.} Different classes have different amounts of within-class variation. For example, the digit ``1'' has consistent appearance, while ``2'' exhibits more variation. We allocate prototypes proportionally to class variance.
\end{enumerate}

Our algorithm: (1) compute within-class variance for each class, (2) allocate prototype budget $M$ proportionally based on variance, (3) use K-Means clustering within each class to find representative samples.

%============================================================================
% SECTION 2: Problem Formulation
%============================================================================
\section{Problem Formulation}
\label{sec:problem}

Given a training set $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N$ where $\mathbf{x}_i \in \mathbb{R}^d$ and $y_i \in \{0, 1, \ldots, C-1\}$, the prototype selection problem is to find a subset $\mathcal{P} \subset \mathcal{D}$ with $|\mathcal{P}| = M \ll N$ that maximizes classification accuracy on a held-out test set when using 1-NN with $\mathcal{P}$ as the reference set.

The 1-NN classifier predicts:
\begin{equation}
    \hat{y} = y_{j^*}, \quad \text{where } j^* = \argmin_{(\mathbf{x}_j, y_j) \in \mathcal{P}} \|\mathbf{x} - \mathbf{x}_j\|_2
\end{equation}

The compression ratio is defined as $N/M$. For MNIST with $N = 60,000$ training samples, selecting $M = 1,000$ prototypes yields 60$\times$ compression.

%============================================================================
% SECTION 3: Pseudocode (Requirement 2)
%============================================================================
\section{Algorithm}
\label{sec:method}

\begin{algorithm}[tb]
   \caption{Variance-Weighted Centroid Selection}
   \label{alg:ours}
\begin{algorithmic}
   \STATE {\bfseries Input:} Training data $\{(\mathbf{x}_i, y_i)\}_{i=1}^N$, budget $M$
   \STATE {\bfseries Output:} Prototype indices $\mathcal{I}$
   \STATE
   \STATE // Step 1: Compute within-class variance
   \FOR{each class $c = 0, \ldots, C-1$}
       \STATE $\boldsymbol{\mu}_c \leftarrow \frac{1}{|D_c|} \sum_{\mathbf{x} \in D_c} \mathbf{x}$ \hfill (class mean)
       \STATE $\sigma_c^2 \leftarrow \frac{1}{|D_c|} \sum_{\mathbf{x} \in D_c} \|\mathbf{x} - \boldsymbol{\mu}_c\|^2$ \hfill (class variance)
   \ENDFOR
   \STATE
   \STATE // Step 2: Allocate prototypes proportionally to variance
   \FOR{each class $c$}
       \STATE $M_c \leftarrow \max\left(1, \text{round}\left(M \cdot \frac{\sigma_c^2}{\sum_{c'} \sigma_{c'}^2}\right)\right)$
   \ENDFOR
   \STATE Adjust $\{M_c\}$ so that $\sum_c M_c = M$
   \STATE
   \STATE // Step 3: Select prototypes via K-Means clustering
   \STATE $\mathcal{I} \leftarrow \emptyset$
   \FOR{each class $c$}
       \STATE Run MiniBatchKMeans on $D_c$ with $M_c$ clusters
       \STATE Let $\{\mathbf{c}_1, \ldots, \mathbf{c}_{M_c}\}$ be the cluster centroids
       \FOR{each centroid $\mathbf{c}_k$}
           \STATE $i^* \leftarrow \argmin_{i: y_i = c} \|\mathbf{x}_i - \mathbf{c}_k\|$
           \STATE $\mathcal{I} \leftarrow \mathcal{I} \cup \{i^*\}$
       \ENDFOR
   \ENDFOR
   \STATE \textbf{return} $\mathcal{I}$
\end{algorithmic}
\end{algorithm}

\textbf{Implementation Details:}
\begin{itemize}
    \item We use MiniBatchKMeans (sklearn) with batch\_size=256, n\_init=3, max\_iter=100 for efficiency.
    \item Time complexity: $O(N \cdot M \cdot t)$ where $t$ is K-Means iterations.
    \item Runtime: 2--680 seconds depending on $M$ (on a standard laptop).
\end{itemize}

%============================================================================
% SECTION 4: Experimental Results (Requirement 3)
%============================================================================
\section{Experiments}
\label{sec:experiments}

\subsection{Dataset and Experimental Setup}

\textbf{Dataset:} MNIST handwritten digits \cite{lecun1998mnist} with 60,000 training and 10,000 test images ($28 \times 28$ pixels, flattened to 784 dimensions, normalized to $[0, 1]$).

\textbf{Baseline:} Random selection (uniformly sampling $M$ training points, ensuring at least one per class).

\textbf{Trials and Error Bars:} All experiments use $n=5$ independent trials with different random seeds. We report mean $\pm$ standard deviation. The 95\% confidence interval is computed as:
\begin{equation}
    \text{CI}_{95\%} = \bar{x} \pm 1.96 \cdot \frac{s}{\sqrt{n}}
\end{equation}
where $\bar{x}$ is the sample mean and $s$ is the sample standard deviation.

\textbf{Upper Bound:} Full 1-NN using all 60,000 training samples achieves \textbf{96.91\%} test accuracy.

\subsection{Main Results}

Table \ref{tab:main-results} shows classification accuracy for $M \in \{10000, 5000, 1000\}$ (required) and additional values. Our method consistently outperforms random selection.

\begin{table}[t]
\caption{Test accuracy (\%) for different prototype budgets $M$. Results are mean $\pm$ std over 5 trials. $\Delta$ = improvement over random.}
\label{tab:main-results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
$M$ & Compress. & Ours & Random & $\Delta$ \\
\midrule
10000 & 6$\times$    & 95.36$\pm$0.13 & 94.76$\pm$0.09 & +0.60 \\
5000  & 12$\times$   & 94.58$\pm$0.21 & 93.63$\pm$0.09 & +0.95 \\
1000  & 60$\times$   & 92.47$\pm$0.17 & 88.56$\pm$0.51 & +3.91 \\
\midrule
500   & 120$\times$  & 91.40$\pm$0.33 & 84.70$\pm$0.28 & +6.70 \\
100   & 600$\times$  & 86.19$\pm$0.40 & 72.39$\pm$0.80 & +13.80 \\
50    & 1200$\times$ & 82.48$\pm$0.62 & 61.57$\pm$4.51 & +20.91 \\
10    & 6000$\times$ & 67.01$\pm$0.33 & 39.37$\pm$9.21 & +27.64 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{main_comparison}}
\caption{Accuracy vs. number of prototypes $M$. Our method (blue) consistently outperforms random selection (orange), with the gap widening at lower $M$.}
\label{fig:main-comparison}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{improvement_bar}}
\caption{Improvement of our method over random selection. The advantage scales dramatically with compression ratio: +0.60\% at $M$=10000 vs. +27.64\% at $M$=10.}
\label{fig:improvement}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Key Observations}

\begin{enumerate}
    \item \textbf{Advantage scales with compression:} At 6$\times$ compression, improvement is modest (+0.60\%). At 6000$\times$ compression, improvement is dramatic (+27.64\%).

    \item \textbf{Stability:} Random selection's variance increases sharply at low $M$ (std=9.21\% at $M$=10), while our method remains stable (std=0.33\%). See Figure \ref{fig:main-comparison}.

    \item \textbf{Graceful degradation:} At 60$\times$ compression ($M$=1000), we achieve 92.47\%, losing only 4.44\% compared to full 1-NN (96.91\%).
\end{enumerate}

%============================================================================
% SECTION 5: Critical Evaluation (Requirement 4)
%============================================================================
\section{Critical Evaluation}
\label{sec:evaluation}

\subsection{Is Our Method a Clear Improvement?}

\textbf{Yes.} Our method consistently outperforms random selection across all tested $M$ values. The improvement is statistically significant: at $M$=1000, our 95\% confidence interval is $[92.32\%, 92.62\%]$ while random's is $[88.11\%, 89.01\%]$---the intervals do not overlap.

The improvement is especially pronounced at high compression ratios, where our method provides \textbf{+27.64\%} accuracy over random selection at $M$=10 (6000$\times$ compression).

\subsection{Scope for Improvement}

Several avenues remain unexplored:

\begin{enumerate}
    \item \textbf{Prototype generation vs. selection:} Our method selects existing training points. Learning Vector Quantization (LVQ) could \emph{generate} optimal prototype positions not constrained to training data.

    \item \textbf{Adaptive $k$ in K-Means:} Currently we fix $M_c$ based on variance. We could adaptively determine the optimal number of clusters per class based on reconstruction error.

    \item \textbf{Cross-class interactions:} Our method processes each class independently. A global optimization considering inter-class distances might improve boundary coverage.

    \item \textbf{Beyond Euclidean distance:} Using learned distance metrics or embedding spaces could better capture similarity structure.
\end{enumerate}

\subsection{What We Tried That Did Not Work}

We experimented with two alternative approaches:

\textbf{Boundary-First Selection:} Select points near class boundaries (those with mixed-class $k$-NN neighborhoods). Result: \textbf{56.77\%} at $M$=500, \emph{worse} than random (84.70\%).

\textbf{Condensed Nearest Neighbor:} Iteratively add misclassified points \cite{hart1968cnn}. Result: \textbf{62.97\%} at $M$=500, also worse than random.

\textbf{Lesson learned:} Boundary points and misclassified points are \emph{atypical} samples---they are ambiguous, noisy, or edge cases. Good prototypes should be \emph{typical, representative} samples, which is exactly what K-Means centroids capture.

\subsection{What We Would Like to Try Next}

\begin{enumerate}
    \item \textbf{Hybrid approach:} Use our variance-weighted selection for most prototypes, but reserve a small budget for boundary-adjacent samples.

    \item \textbf{Deep embeddings:} Apply our method in a learned feature space (e.g., CNN embeddings) rather than raw pixels.

    \item \textbf{Other datasets:} Evaluate on CIFAR-10, Fashion-MNIST, or higher-dimensional datasets.
\end{enumerate}

%============================================================================
% SECTION 6: Related Work
%============================================================================
\section{Related Work}
\label{sec:related}

Prototype selection has been studied extensively. The Condensed Nearest Neighbor (CNN) rule \cite{hart1968cnn} iteratively builds a consistent subset. Edited Nearest Neighbor (ENN) \cite{wilson1972asymptotic} removes noisy samples. More recent work includes genetic algorithms \cite{cano2003using} and deep learning approaches \cite{snell2017prototypical}.

Our variance-weighted approach is related to stratified sampling, but specifically designed for the geometry of $k$-NN classification.

%============================================================================
% SECTION 7: Conclusion
%============================================================================
\section{Conclusion}

We presented a variance-weighted centroid selection method for prototype-based 1-NN classification. The key insight is that \textbf{good prototypes are typical samples}, not boundary samples. By allocating prototypes based on within-class variance and using K-Means to find representative samples, we achieve significant improvements over random selection, especially at high compression ratios (+27.64\% at 6000$\times$ compression).

\textbf{Practical Recommendation:} For high accuracy, use $M \geq 1000$ (60$\times$ compression, 92.47\%). For extreme compression, $M = 50$ (1200$\times$) still achieves 82.48\%.

%============================================================================
% Acknowledgements
%============================================================================
\section*{Acknowledgements}

This project was developed with assistance from Claude (Anthropic), a large language model, used as an interactive coding assistant for algorithm design, implementation, debugging, and documentation. The human contributor provided problem formulation, direction, critical evaluation, and all final decisions. All experimental results were computed on actual hardware.

%============================================================================
% Impact Statement
%============================================================================
\section*{Impact Statement}

This paper presents work to advance efficient nearest-neighbor classification. We do not foresee specific negative societal consequences.

\bibliography{references}
\bibliographystyle{icml2025}

\end{document}
