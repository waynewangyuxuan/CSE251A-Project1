%%%%%%%% ICML 2025 STYLE REPORT %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended packages
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Camera-ready style
\usepackage[accepted]{icml2025}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Define argmin
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[capitalize,noabbrev]{cleveref}

% Algorithm packages
\usepackage{algorithm}
\usepackage{algorithmic}

% Theorems
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\icmltitlerunning{Prototype Selection for 1-NN Classification}

\begin{document}

\twocolumn[
\icmltitle{Variance-Weighted Centroid Selection for \\
           Prototype-Based 1-Nearest Neighbor Classification}

\begin{icmlauthorlist}
\icmlauthor{Wayne Wang}{ucsd}
\end{icmlauthorlist}

\icmlaffiliation{ucsd}{Department of Computer Science and Engineering, University of California San Diego, La Jolla, CA, USA}

\icmlcorrespondingauthor{Wayne Wang}{waw009@ucsd.edu}

\icmlkeywords{Prototype Selection, Nearest Neighbor, MNIST, Clustering}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
We address the problem of selecting a small set of representative prototypes from a large training set for 1-nearest neighbor (1-NN) classification. We propose a variance-weighted centroid selection algorithm that allocates prototypes proportionally based on within-class variance and uses K-Means clustering to identify representative samples. Experiments on MNIST demonstrate that our method significantly outperforms random selection, with advantages becoming more pronounced at higher compression ratios. At 60$\times$ compression (1,000 prototypes from 60,000 training samples), our method achieves 92.47\% accuracy compared to 88.56\% for random selection. At extreme compression (6,000$\times$, only 10 prototypes), our method maintains 67.01\% accuracy while random selection degrades to 39.37\%. We also report negative results from two alternative approaches---boundary-first selection and condensed nearest neighbor---providing insights into why typical samples make better prototypes than boundary samples.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The $k$-nearest neighbor ($k$-NN) algorithm is a fundamental non-parametric classifier that predicts the label of a test sample based on the labels of its $k$ closest training samples \cite{cover1967nearest}. Despite its simplicity and strong theoretical guarantees, $k$-NN suffers from high computational cost at inference time, as it requires computing distances to all training samples.

\textbf{Prototype selection} addresses this limitation by selecting a representative subset of training samples, called prototypes, to use for classification instead of the full training set. The goal is to maintain classification accuracy while dramatically reducing storage and computational requirements.

In this work, we focus on prototype selection for 1-NN classification on MNIST handwritten digits. Our key contributions are:

\begin{enumerate}
    \item A \textbf{variance-weighted centroid selection} algorithm that allocates prototypes proportionally based on within-class variance and uses K-Means to identify representative samples.
    \item Comprehensive experiments showing our method's advantage increases with compression ratio, achieving +27.64\% improvement over random selection at 6,000$\times$ compression.
    \item \textbf{Negative results} demonstrating that boundary points and misclassified points are poor prototype candidates, contrary to intuition.
\end{enumerate}

\section{Problem Formulation}
\label{sec:problem}

Given a training set $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N$ where $\mathbf{x}_i \in \mathbb{R}^d$ and $y_i \in \{0, 1, \ldots, C-1\}$, the prototype selection problem is to find a subset $\mathcal{P} \subset \mathcal{D}$ with $|\mathcal{P}| = M \ll N$ that maximizes classification accuracy on a held-out test set when using 1-NN with $\mathcal{P}$ as the reference set.

The 1-NN classifier predicts:
\begin{equation}
    \hat{y} = y_{j^*}, \quad \text{where } j^* = \argmin_{(\mathbf{x}_j, y_j) \in \mathcal{P}} \|\mathbf{x} - \mathbf{x}_j\|_2
\end{equation}

The compression ratio is defined as $N/M$. For MNIST with $N = 60,000$ training samples, selecting $M = 1,000$ prototypes yields 60$\times$ compression.

\section{Proposed Method}
\label{sec:method}

\subsection{Key Insight}

Our method is based on the observation that \textbf{good prototypes are typical, representative samples}---not boundary samples. While decision boundaries are defined by samples near class interfaces, the best prototypes are cluster centroids that represent the ``typical'' appearance of each class.

Different classes may have different amounts of within-class variation. For example, in MNIST, the digit ``1'' has relatively consistent appearance, while ``2'' and ``4'' exhibit more variation. Classes with higher variance require more prototypes to adequately cover their data distribution.

\subsection{Algorithm}

Our variance-weighted centroid selection algorithm proceeds as follows:

\begin{algorithm}[tb]
   \caption{Variance-Weighted Centroid Selection}
   \label{alg:ours}
\begin{algorithmic}
   \STATE {\bfseries Input:} Training data $\{(\mathbf{x}_i, y_i)\}_{i=1}^N$, budget $M$
   \STATE {\bfseries Output:} Prototype indices $\mathcal{I}$
   \STATE
   \STATE // Step 1: Compute within-class variance
   \FOR{each class $c = 0, \ldots, C-1$}
       \STATE $\sigma_c^2 \leftarrow \frac{1}{|D_c|} \sum_{\mathbf{x} \in D_c} \|\mathbf{x} - \boldsymbol{\mu}_c\|^2$
   \ENDFOR
   \STATE
   \STATE // Step 2: Allocate prototypes proportionally
   \FOR{each class $c$}
       \STATE $M_c \leftarrow \max\left(1, \text{round}\left(M \cdot \frac{\sigma_c^2}{\sum_{c'} \sigma_{c'}^2}\right)\right)$
   \ENDFOR
   \STATE Adjust allocations to sum to exactly $M$
   \STATE
   \STATE // Step 3: Select prototypes via K-Means
   \STATE $\mathcal{I} \leftarrow \emptyset$
   \FOR{each class $c$}
       \STATE Run MiniBatchKMeans on $D_c$ with $M_c$ clusters
       \STATE For each centroid, add index of nearest point to $\mathcal{I}$
   \ENDFOR
   \STATE \textbf{return} $\mathcal{I}$
\end{algorithmic}
\end{algorithm}

\textbf{Complexity:} The algorithm runs in $O(N \cdot M \cdot t)$ time where $t$ is the number of K-Means iterations. Using MiniBatchKMeans with batch size 256 and $t=100$ iterations, selection takes 2--680 seconds depending on $M$.

\section{Experiments}
\label{sec:experiments}

\subsection{Dataset and Setup}

We evaluate on the MNIST handwritten digit dataset \cite{lecun1998mnist} containing 60,000 training and 10,000 test images of size $28 \times 28$ pixels. Images are flattened to 784-dimensional vectors and normalized to $[0, 1]$.

We compare our method against random selection (ensuring at least one sample per class). All experiments use 5 trials with different random seeds, reporting mean $\pm$ standard deviation.

\subsection{Baseline: Full 1-NN}

Using all 60,000 training samples, 1-NN achieves \textbf{96.91\%} test accuracy. This serves as the upper bound for prototype selection methods.

\subsection{Main Results}

Table \ref{tab:main-results} shows classification accuracy across different compression ratios. Our method consistently outperforms random selection, with the advantage growing dramatically as compression increases.

\begin{table}[t]
\caption{Test accuracy (\%) for different prototype budgets $M$. ``Ours'' refers to variance-weighted centroid selection.}
\label{tab:main-results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
$M$ & Compress. & Ours & Random & $\Delta$ \\
\midrule
10000 & 6$\times$    & 95.36$\pm$0.13 & 94.76$\pm$0.09 & +0.60 \\
5000  & 12$\times$   & 94.58$\pm$0.21 & 93.63$\pm$0.09 & +0.95 \\
2500  & 24$\times$   & 93.62$\pm$0.19 & 91.96$\pm$0.25 & +1.66 \\
1000  & 60$\times$   & 92.47$\pm$0.17 & 88.56$\pm$0.51 & +3.91 \\
500   & 120$\times$  & 91.40$\pm$0.33 & 84.70$\pm$0.28 & +6.70 \\
250   & 240$\times$  & 89.71$\pm$0.34 & 80.27$\pm$0.36 & +9.44 \\
100   & 600$\times$  & 86.19$\pm$0.40 & 72.39$\pm$0.80 & +13.80 \\
50    & 1200$\times$ & 82.48$\pm$0.62 & 61.57$\pm$4.51 & +20.91 \\
25    & 2400$\times$ & 77.11$\pm$0.91 & 49.76$\pm$5.80 & +27.35 \\
10    & 6000$\times$ & 67.01$\pm$0.33 & 39.37$\pm$9.21 & +27.64 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Key observations:
\begin{itemize}
    \item \textbf{Advantage scales with compression:} At low compression (6$\times$), improvement is modest (+0.60\%). At extreme compression (6000$\times$), improvement is dramatic (+27.64\%).
    \item \textbf{Stability:} Random selection's variance increases sharply at low $M$ (std=9.21\% at $M$=10), while our method remains stable (std=0.33\%).
    \item \textbf{Graceful degradation:} At 60$\times$ compression, we lose only 4.44\% accuracy compared to full 1-NN.
\end{itemize}

\subsection{Negative Results: Failed Approaches}

We also experimented with two alternative approaches that performed poorly:

\textbf{Boundary-First Selection:} We hypothesized that points near class boundaries are most important for defining decision surfaces. We identified boundary points (those with mixed-class $k$-NN neighborhoods) and performed K-Means only on these boundary points. Result: \textbf{56.77\%} accuracy at $M$=500, worse than random (84.70\%).

\textbf{Condensed Nearest Neighbor (CNN):} Inspired by the classic CNN algorithm \cite{hart1968cnn}, we iteratively added misclassified points to the prototype set. Result: \textbf{62.97\%} accuracy at $M$=500, also worse than random.

\textbf{Analysis:} Both methods fail because they select atypical samples. Boundary points are often ambiguous or noisy examples. Misclassified points are, by definition, hard to classify. Neither makes a good prototype. This confirms our key insight: \textbf{good prototypes are typical samples, not boundary samples}.

\section{Related Work}
\label{sec:related}

Prototype selection has been studied extensively. The Condensed Nearest Neighbor (CNN) rule \cite{hart1968cnn} iteratively builds a consistent subset. Edited Nearest Neighbor (ENN) \cite{wilson1972asymptotic} removes noisy samples. More recent work includes genetic algorithms \cite{cano2003using}, particle swarm optimization \cite{nanni2009particle}, and deep learning approaches \cite{snell2017prototypical}.

Our variance-weighted approach is related to stratified sampling and importance sampling, but specifically designed for the geometry of $k$-NN classification.

\section{Conclusion}
\label{sec:conclusion}

We presented a simple yet effective prototype selection method for 1-NN classification. By allocating prototypes based on within-class variance and using K-Means to identify representative samples, we achieve significant improvements over random selection, especially at high compression ratios.

Our negative results with boundary-first and CNN-inspired approaches provide a valuable lesson: for prototype selection, \textbf{typical samples are better than boundary samples}. The best prototypes are cluster centroids that represent the ``normal'' appearance of each class, not edge cases.

\textbf{Practical Recommendation:} For applications requiring high accuracy, use $M \geq 1000$ (60$\times$ compression, 92.47\% accuracy). For storage-constrained applications, even $M = 50$ (1200$\times$ compression) maintains 82.48\% accuracy.

\section*{Acknowledgements}

This project was developed with assistance from Claude (Anthropic), a large language model. The LLM was used as an interactive coding assistant for:

\begin{itemize}
    \item \textbf{Algorithm design:} Brainstorming and iterating on prototype selection approaches, including the variance-weighted allocation scheme.
    \item \textbf{Implementation:} Writing Python code for data loading, prototype selection algorithms, experiment framework, and visualization.
    \item \textbf{Debugging:} Identifying performance bottlenecks (e.g., switching from standard K-Means to MiniBatchKMeans).
    \item \textbf{Documentation:} Maintaining progress logs and generating this report.
\end{itemize}

The human contributor provided problem formulation, high-level direction, critical evaluation of results, and all final decisions. The LLM served as a ``pair programming'' partner, accelerating implementation while the human maintained intellectual oversight.

This collaboration exemplifies a productive human-AI workflow: the human provides domain expertise and judgment, while the AI accelerates routine tasks and suggests alternatives. All experimental results were computed on actual hardware; no results were fabricated.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. The technique of prototype selection can reduce computational costs and storage requirements for nearest-neighbor classifiers, potentially enabling deployment on resource-constrained devices. We do not foresee specific negative societal consequences from this work.

\bibliography{references}
\bibliographystyle{icml2025}

\end{document}
